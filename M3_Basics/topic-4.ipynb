{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Topic 4\n",
    "\n",
    "**Regression** - **Регрессия**\n",
    "\n",
    "Линейная и логистическая регрессия — два широко используемых алгоритма машинного обучения, которые оба являются примерами обучения с учителем. \n",
    "\n",
    "Линейную регрессию можно использовать для прогнозирования непрерывной зависимой переменной по шкале значений.\n",
    "Логистическая регрессия используется, когда ожидается результат бинарной операции (например, «да» или «нет»).\n",
    "\n",
    "### Линейная регрессия\n",
    "\n",
    "Линейная регрессия предполагает, что между зависимыми и независимыми переменными существует линейная связь. Цель линейной регрессии — найти наиболее подходящую линию/плоскость, которая может точно предсказать результат для непрерывных зависимых переменных, например, цены на жилье, возраст.\n",
    "\n",
    "Простая (парная) линейная регрессия — это модель регрессии, которая оценивает взаимосвязь между одной независимой переменной и одной зависимой переменной с помощью прямой линии.\n",
    "Множественная линейная регрессия  — если независимых переменных более двух.\n",
    "\n",
    "### Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия — это модель классификации, которая использует входные переменные (признаки) для прогнозирования категориальной выходной переменной, которая может принимать одно из ограниченного набора значений класса.\n",
    "\n",
    "Биномиальная логистическая регрессия ограничена двумя категориями двоичных выходных данных.\n",
    "Полиномиальная логистическая регрессия допускает более двух классов.\n",
    "\n",
    "\n",
    "1. Что такое регрессия. Введение.\n",
    "2. Линейная регрессия. Постановка задачи.\n",
    "3. Предположения линейной регрессии.\n",
    "4. Парная (простая) линейная регрессия: критерий оптимальности, метод наименьших квадратов, примеры, проверка предположений.\n",
    "5. Парная нелинейная регрессия, примеры.\n",
    "6. Множественная линейная регрессия: аналитическое решение, сингулярное разложение, приближенное решение, стохастический градиентный спуск.\n",
    "7. Регуляризация: гребневая регрессия, регрессия LASSO, эластичные сети.\n",
    "8. Метрики качества линейной регрессии.\n",
    "9. Множественная линейная регрессия: ранжирование признаков, ошибки прогноза, визуализация остатков.\n",
    "10. Классификаторы для задачи регрессии: регрессия SVM, регрессия на основе KNN, регрессия на основе CART, регрессия на основе Random Forest.\n",
    "11. Линейная регрессия. Применение.\n",
    "12. Логистическая регрессия: введение, типы логистической регрессии.\n",
    "13. Связь линейной и логистической регрессий\n",
    "14. Метод максимального правдоподобия: максимизация функции правдоподобия, градиентный спуск, метод Ньютона-Рафсона.\n",
    "15. Регуляризация логистической регрессии.\n",
    "16. Бинарная логистическая регрессия.\n",
    "17. Мультиномиальная логистическая регрессия.\n",
    "18. Ординальная логистическая регрессия.\n",
    "19. Логистическая регрессия. Применение.\n",
    "20. Отбор информативных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset. Take the first two features\n",
    "diabetes = load_diabetes()\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Create train and test split\n",
    "X_train = diabetes_X[:-40]\n",
    "X_test = diabetes_X[-40:]\n",
    "y_train = diabetes.target[:-40]\n",
    "y_test = diabetes.target[-40:]\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a non -linear dataset with a quadratic relationship\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-10, 10, 100).reshape(-1, 1)\n",
    "y = 0.5 * x ** 2 + np.random.normal(0, 20, size=100)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Fit linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x, y)\n",
    "y_pred_linear = linear_model.predict(x)\n",
    "\n",
    "# Fit polynomial regression model(degree = 2)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "x_poly = poly_features.fit_transform(x)\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(x_poly, y)\n",
    "y_pred_poly = poly_model.predict(x_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing ()\n",
    "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "data['AveCostHouse'] = housing.target.tolist()\n",
    "\n",
    "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "X = data[['MedInc', 'AveBedrms', 'AveRooms', 'Latitude', 'Longitude']]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "sc = StandardScaler().fit(X)\n",
    "Xtrain_std, Xtest_std = sc.transform(Xtrain), sc.transform(Xtest)\n",
    "\n",
    "modelRidge = Ridge(alpha=10.0, fit_intercept=True)\n",
    "modelRidge.fit(Xtrain_std, ytrain)\n",
    "\n",
    "yR = modelRidge.predict(Xtest_std)\n",
    "RMSE_Ridge = mean_squared_error(ytest, yR, squared=False)\n",
    "MAE_Ridge = mean_absolute_error(ytest, yR)\n",
    "R2_Riddge = r2_score(ytest, yR)\n",
    "\n",
    "alphasR = np.linspace(0.1, 10.0, num=1000)\n",
    "modelRidgeCV = RidgeCV(alphas=alphasR)\n",
    "modelRidgeCV.fit(Xtrain_std, ytrain)\n",
    "yRCV = modelRidgeCV.predict(Xtest_std)\n",
    "RMSE_RidgeCV = mean_squared_error(ytest, yRCV, squared=False)\n",
    "MAE_RidgeCV = mean_absolute_error(ytest, yRCV)\n",
    "R2_RiddgeCV = r2_score(ytest, yRCV)\n",
    "\n",
    "modelLasso = Lasso(alpha=0.3, fit_intercept=True)\n",
    "modelLasso.fit(Xtrain_std, ytrain)\n",
    "yL = modelLasso.predict(Xtest_std)\n",
    "RMSE_Lasso = mean_squared_error(ytest, yL, squared=False)\n",
    "MAE_Lasso = mean_absolute_error(ytest, yL)\n",
    "R2_Lasso = r2_score(ytest, yL)\n",
    "###\n",
    "alphasL = np.linspace(0.1, 2.0, num =1000)\n",
    "modelLassoCV = LassoCV(cv=5, random_state=0)\n",
    "modelLassoCV.fit(Xtrain_std, ytrain)\n",
    "yLCV = modelLassoCV.predict(Xtest_std)\n",
    "RMSE_LassoCV = mean_squared_error(ytest, yLCV, squared=False)\n",
    "MAE_LassoCV = mean_absolute_error(ytest, yLCV)\n",
    "R2_LassoCV = r2_score(ytest, yLCV)\n",
    "\n",
    "modelElast = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "modelElast.fit(Xtrain_std, ytrain)\n",
    "yE = modelElast.predict(Xtest_std)\n",
    "RMSE_Elast = mean_squared_error(ytest, yE, squared=False)\n",
    "MAE_Elast = mean_absolute_error(ytest, yE)\n",
    "R2_Elast = r2_score(ytest , yE)\n",
    "###\n",
    "modelElastCV = ElasticNetCV(cv=5, random_state=0)\n",
    "modelElastCV.fit(Xtrain_std, y_train)\n",
    "yECV = modelElastCV.predict(Xtest_std)\n",
    "RMSE_ElastCV = mean_squared_error(ytest, yECV, squared=False)\n",
    "MAE_ElastCV = mean_absolute_error(ytest, yECV)\n",
    "R2_ElastCV = r2_score(ytest, yECV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler().fit(X)\n",
    "X_std = sc.transform(X)\n",
    "X_train_std, X_test_std = sc.transform(X_train), sc.transform(X_test)\n",
    "\n",
    "# Binomial Logistic regression. Fit the model\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test_std)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset . Take the first two features\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [0 ,1]], iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit the model (lbfgs - L2 regularization, Multinomial regression)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

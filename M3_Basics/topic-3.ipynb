{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Topic 3\n",
    "\n",
    "**Classification** - **Классификация**\n",
    "\n",
    "Классификация — это обучение с учителем, которое используется для определения классов новых наблюдений на основе обучающих данных.\n",
    "При классификации модель обучается на основе данного набора данных, а затем классифицирует новые наблюдения по нескольким классам.\n",
    "\n",
    "В классификации машинного обучения есть два типа алгоритмов: ленивые и нетерпеливые (энергичные).\n",
    "\n",
    "«Нетерпеливые» — это алгоритмы машинного обучения, которые сначала строят модель на основе обучающего набора данных,прежде чем делать какие-либо прогнозы для будущих наборов данных.Они тратят больше времени на тренировочный процесс, но им требуется меньше времени для прогнозирования.  Примеры: метод опорных векторов, деревья решений, наивный Байес.\n",
    "«Ленивые»  алгоритмы не создают какую-либо модель сразу на основе обучающих данных. Они просто запоминают данные обучения, и каждый раз, когда возникает необходимость сделать прогноз, они ищут ближайшего соседа из всех данных обучения, что делает их очень медленными во время прогнозирования. Пример:  метод К-ближайших соседей.\n",
    "\n",
    "\n",
    "1. Формальная постановка задачи классификации.\n",
    "2. Переобучение и недообучение.\n",
    "3. Метод опорных векторов: идея, выбор гиперплоскости, линейная неразделимость, спрямляющее отображение, трюк с ядром, часто используемые ядра.\n",
    "4. Мультиклассовый метод опорных векторов: один против остальных, каждый против каждого.\n",
    "5. Классификатор к-ближайших соседей (KNN): взвешенный KNN, достоинства и недостатки, выбор гиперпараметра k, выбор гиперпараметров с помощью GridSearch.\n",
    "6. Деревья решений: основные понятия, примеры, построение дерева решений, критерий Джини, критерий прироста информации (ID3).\n",
    "7. Алгоритм CART.\n",
    "8. Ансамбли. Random Forest: примеры, оптимальное число деревьев, выбор гиперпараметров с помощью GridSearch.\n",
    "9. Байесовский классификатор. Теорема Байеса.\n",
    "10. Наивный байесовский классификатор: номинальные признаки, классификация спама, типы наивных байесовских классификаторов, примеры, преимущества и недостатки.\n",
    "11. Оценка качества в бинарной классификации: матрица ошибок, Accuracy, Balanced Accuracy, Precision, Recall, F-score, ROC-кривая, AUC.\n",
    "12. Оценка качества многоклассовой классификации.\n",
    "13. Кросс-валидация.\n",
    "14. Балансировка классов: сокращение мажоритарного класса, увеличение миноритарного класса.\n",
    "15. Применения методов классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_circles, make_gaussian_quantiles, load_iris, make_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data set\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "Y = [0] * 20 + [1] * 20\n",
    "\n",
    "# Fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Draw the separating hypoplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# Construct lines through the reference vectors\n",
    "b = clf.support_vectors_[0]\n",
    "yy_down = a * xx + (b[1] - a * b[0])\n",
    "b = clf.support_vectors_[-1]\n",
    "yy_up = a * xx + (b[1] - a * b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "X2, y2 = make_gaussian_quantiles(n_features=2, n_classes=2, n_samples=1000, mean=(2, 3))\n",
    "\n",
    "linear_svc = svm.SVC(kernel='linear').fit(X1, y1)\n",
    "linear_svc = svm.SVC(kernel='linear').fit(X2, y2)\n",
    "\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=1).fit(X1, y1)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=1).fit(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris();\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr ), sc.transform(X_t)\n",
    "\n",
    "# Fit the model and define strategy and fit model\n",
    "ovr = OneVsRestClassifier(svm.SVC()).fit(X_tr_std, y_tr)\n",
    "\n",
    "# make predictions\n",
    "y_ovr = ovr.predict(X_t_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset.\n",
    "iris = load_iris();\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "# define strategy and fit model\n",
    "ovo = OneVsOneClassifier(svm.SVC()).fit(X_tr_std, y_tr)\n",
    "# make predictions\n",
    "y_ovo = ovo.predict(X_t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset. Take the first two features\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [0 ,1]], iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "# Fit the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, weights= 'uniform', algorithm='auto').fit(X_tr_std, y_tr)\n",
    "predicted = knn.predict(X_t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "error_rate = []\n",
    "for i in range(1 ,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i).fit(X_tr_std , y_tr)\n",
    "    pred_i = knn.predict(X_t_std)\n",
    "    error_rate.append(np.mean(pred_i != y_t))\n",
    "\n",
    "\n",
    "grid_params = {\n",
    "    'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "gs = GridSearchCV(knn, grid_params, scoring='accuracy', refit=True)\n",
    "\n",
    "g_res = gs.fit(X_tr_std, y_tr)\n",
    "print(g_res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset. Take the first two features\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [0 ,1]], iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "# Fit the model\n",
    "model = DecisionTreeClassifier().fit(X_tr, y_tr)\n",
    "pred_y = model.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset. Take the first two features\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [0 ,1]], iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "# Fit the model(n_estimators - Number of trees)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_tr_std, y_tr)\n",
    "y_pred = clf.predict(X_t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr_std, X_t_std = sc.transform(X_tr), sc.transform(X_t)\n",
    "\n",
    "error_rate = []\n",
    "for i in range(100 ,200):\n",
    "    cl = RandomForestClassifier(n_estimators=i)\n",
    "    cl.fit(X_tr_std, y_tr)\n",
    "    pred_i = cl.predict(X_t_std)\n",
    "    error_rate.append(np.mean(pred_i != y_t))\n",
    "\n",
    "rfc = RandomForestClassifier(random_state =42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100 , 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 5, 6, 7, 8],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring='accuracy', refit=True)\n",
    "g_res = CV_rfc.fit(X_tr_std, y_tr)\n",
    "print(g_res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset. Take the first two features\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [0 ,1]], iris.target\n",
    "\n",
    "# Create train and test split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_tr, y_tr).predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['This is a sample document.', 'Another document to test.', 'A third sample for testing.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, [0, 1, 0])\n",
    "\n",
    "new_doc = ['This is another test document.']\n",
    "new_X = vectorizer.transform(new_doc)\n",
    "predicted_class = clf.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "model = GaussianNB()\n",
    "model.fit(X,y)\n",
    "y_score = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculation fpr = False Positive Rate , tpr = True Positive Rate and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y, y_score)\n",
    "AUC = auc(fpr , tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Load model\n",
    "rfm = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# 1. Hold-out cross-validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "rfm.fit(X_train, y_train)\n",
    "\n",
    "# 2. K-fold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "score = cross_val_score(rfm, X, y, cv=kf)\n",
    "\n",
    "# 3. Stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "score = cross_val_score(rfm, X, y, cv=skf)\n",
    "\n",
    "# 4. Leave-One-Out Cross-Validation\n",
    "LOOCV= LeaveOneOut()\n",
    "score = cross_val_score(rfm, X, y, cv=LOOCV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
